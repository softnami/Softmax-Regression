"use strict";


class Softmax {

 /**
   * This method serves as the constructor for the Softmax class.
   * @method constructor
   * @param {Object} args These are the required arguments.
   */
  constructor(args) {
    if (args.notify_count === undefined || args.threshold === undefined || args.learningRate === undefined || args.parameter_size === undefined || args.max_iterations === undefined || args.iteration_callback === undefined) {
      throw ({
        'name': "InvalidParam",
        'message': "The required constructor parameters cannot be empty."
      });
    }
    this.MathJS = require('mathjs');
    this.initArgs = args;
    this.notify_count = this.initArgs.notify_count || 100;
    this.max_iterations = this.initArgs.max_iterations || 1000;
    this.iteration_callback = this.initArgs.iteration_callback;
    this.parameter_size = this.initArgs.parameter_size;
    this.learningRate = this.initArgs.learningRate;
    this.threshold = this.initArgs.threshold;
    this.random_vals = {};
    this.X = {};
    this.Y = {};
    this.W = {};
  }

  /**
   * This method serves as the logic for generating the natural exponent matrix in softmax regression.
   *
   * @method exp_matrix
   * @param {matrix} W The matrix to be used as the weights for the exp_matrix.
   * @param {matrix} X The matrix to be used as the input data for the exp_matrix.
   * @return {matrix} Returns the exp_term matrix.
   */
  exp_matrix(W, X) {

    var scope = {
        x: X[0],
        W: (typeof(W) === "number") ? this.MathJS.matrix([
          [W]
        ]) : this.W,
        W_transpose: {}
      },
      exp_term;

    scope.W_transpose = this.MathJS.transpose(scope.W);
    exp_term = this.MathJS.eval('e.^(W_transpose*x)', scope);

    return exp_term;
  }

  /**
   * This method serves as the logic for hypothesis.
   *
   * @method hypothesis
   * @param {matrix} exp_term The matrix to be used as the exp_term.
   * @return {matrix} Returns the hypothesis matrix.
   */
  hypothesis(exp_term) {
    var sum = this.MathJS.sum(exp_term);
    var scope = {
      exp_term: exp_term,
      sum: sum
    };
    var result = this.MathJS.eval('exp_term.*(1/sum)', scope);

    return result;
  }

  /**
   * This method serves as the logic for the costFunction.
   *
   * @method costFunction
   * @param {matrix} W The matrix to be used as the weights.
   * @param {matrix} X The matrix to be used as the input data.
   * @param {matrix} Y The matrix to be used as the output data.
   * @return {Numer} Returns the cost.
   */
  costFunction(W, X, Y) {

    var cost = 0;
    var W = W || this.W;
    var X = X || this.X;
    var Y = X || this.Y;

    var scope = {};
    scope.previous_cost = this.MathJS.zeros(W.size()[1], W.size()[0]);

    for (let i = 0; i < X.size()[0]; i++) {
      scope.y = Y._data[i][0];
      scope.exp_matrix = this.exp_matrix(W, X._data[i]);
      scope.hypothesis = this.MathJS.matrix(this.hypothesis(scope.exp_matrix));

      scope.log_term = this.MathJS.log(scope.hypothesis);
      scope.log_diff_term = this.MathJS.log(this.MathJS.eval('1-hypothesis', scope));
      scope.cost_current = this.MathJS.eval('((1-y)*log_diff_term+y*log_term)', scope);
      scope.cost = this.MathJS.eval('(cost_current+previous_cost)', scope);
      scope.previous_cost = scope.cost;
    }

    scope.size = X.size()[0];

    var finalcost = this.MathJS.eval('-1*cost./size', scope);

    return this.MathJS.sum(finalcost);
  }

  /**
   * This method serves as the logic for the cost gradient function.
   *
   * @method costFunctionGradient
   * @param {matrix} _W The matrix to be used as the weights.
   * @param {matrix} _X The matrix to be used as the input data.
   * @param {matrix} _Y The matrix to be used as the outut data.

   * @return {matrix} Returns the cost gradient.
   */
  costFunctionGradient(_W, _X, _Y) {
    var cost = 0;
    var gradient = [],
      probability_matrx = [
        []
      ],
      probability_matrx_denominator = [
        []
      ],
      probability_matrx_numerator = [
        []
      ],
      probability_matrx_denominator_sums = [];
    var X = _X || this.X;
    var Y = _Y || this.Y;
    var W = _W || this.W;


    for (let j = 0; j < X.size()[0]; j++) {
      probability_matrx_denominator[j] = [];
      for (let i = 0; i < W.size()[0]; i++) {
        let scope = {};
        scope.W_transpose = this.MathJS.transpose(this.MathJS.matrix(W._data[i]));
        scope.x = (X._data[j][0]);
        probability_matrx_denominator[j][i] = this.MathJS.exp(this.MathJS.eval('W_transpose*x', scope));
      }
    }

    for (let i = 0; i < X.size()[0]; i++) {
      probability_matrx_denominator_sums[i] = this.MathJS.sum(this.MathJS.matrix(probability_matrx_denominator[i]));
    }

    for (let j = 0; j < X.size()[0]; j++) {
      probability_matrx_numerator[j] = [];
      for (let i = 0; i < W.size()[0]; i++) {
        let scope = {};
        scope.W_transpose = this.MathJS.transpose(this.MathJS.matrix(W._data[i]));
        scope.x = (X._data[j][0]);
        probability_matrx_numerator[j][i] = this.MathJS.exp(this.MathJS.eval('W_transpose*x', scope));
      }
    }

    for (let j = 0; j < X.size()[0]; j++) {
      probability_matrx[j] = [];
      for (let i = 0; i < W.size()[0]; i++) {

        let scope = {};
        scope.probability_matrx_denominator_sums = probability_matrx_denominator_sums[j];
        scope.probability_matrx_numerator = probability_matrx_numerator[j][i];
        probability_matrx[j][i] = this.MathJS.eval('probability_matrx_numerator./probability_matrx_denominator_sums', scope)._data;
      }
    }


    for (let j = 0; j < W.size()[0]; j++) {
      let scope = {};
      scope.gradient_previous = this.MathJS.zeros(W.size()[0], W.size()[1]);
      scope.gradient = [];
      scope.gradient[j] = [];
      for (let i = 0; i < X.size()[0]; i++) {
        scope.x = X._data[i][0];
        if (Y._data[i][0] === j) {
          scope.probability_matrx = (probability_matrx[i]);
          scope.gradient[j][i] = this.MathJS.eval('(1-probability_matrx).*x', scope);
        } else if (Y._data[i][0] !== j) {
          scope.probability_matrx = (probability_matrx[i]);
          scope.gradient[j][i] = this.MathJS.eval('(probability_matrx*-1).*x', scope);
        }



        scope.gradient_current = this.MathJS.matrix(scope.gradient[j][i]);
        gradient[j] = this.MathJS.floor(this.MathJS.eval('(gradient_current+gradient_previous)', scope))._data;
        scope.gradient_previous = gradient[j];
      }
    }


    let scope = {};
    scope.gradient = gradient[0];
    scope.size = X.size()[0];

    return this.MathJS.eval('-1*gradient./size', scope);
  }

  /**
   * This method serves as the logic for the gradient descent.
   *
   * @method startRegression
   * @param {matrix} Y The matrix to be used as the ouput data for training.
   * @param {matrix} X The matrix to be used as the input data for training.
   */
  startRegression(X, Y) {

    var iterations = 0;

    this.X = X;
    this.Y = Y;
    var self = this;
    this.W = this.MathJS.random(this.MathJS.matrix([self.parameter_size[1],self.parameter_size[0]]), -1, 1);

    while (true) {

      var scope = {};
      scope.W = this.W;
      scope.cost = this.costFunction();
      scope.gradient = this.costFunctionGradient();
      scope.iterations = iterations;
      scope.learningRate = this.learningRate;


      this.W = this.MathJS.eval('W-(gradient*learningRate)', scope);

      if (iterations % this.notify_count === 0) {
        this.iteration_callback(scope);
      }

      iterations++;

      if(iterations>this.max_iterations || scope.cost < this.threshold){
        console.log("\nTraining completed.\n");
        break;
      }

    }

  }

}

var mathjs = require('mathjs');

var callback = function(data) {

};

var sft = new Softmax({
  'notify_count': 1,
  'parameter_size': [2, 5],
  'max_iterations': 1000,
  'threshold': 0.1,
  'iteration_callback': callback,
  'learningRate': 0.3
});


sft.startRegression(mathjs.floor(mathjs.random(mathjs.matrix([50, 1]), 1, 3)), mathjs.floor(mathjs.random(mathjs.matrix([50, 1]), 1, 5)));
